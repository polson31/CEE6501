{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "title-slide",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "# CEE6501 — Lecture 2.2\n",
        "\n",
        "## Solving Linear Systems: $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "learning-objectives",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "By the end of this lecture, you will be able to:\n",
        "- State why **explicit inversion** is usually a bad idea for solving $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$\n",
        "- Explain and apply **direct elimination** methods (GE, Gauss–Jordan, LU, Cholesky, LDL$^T$, Thomas)\n",
        "- Apply basic **iterative** solvers (Jacobi, Gauss–Seidel) and interpret convergence\n",
        "- Connect matrix **structure** (symmetry, positive definiteness, bandedness) to solver choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "setup-code",
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(precision=4, suppress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "context-slide",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### The Core Problem\n",
        "\n",
        "In matrix structural analysis, everything reduces to solving:\n",
        "\n",
        "$$\n",
        "\\mathbf{K}\\mathbf{u}=\\mathbf{f}\n",
        "$$\n",
        "\n",
        "- $\\mathbf{K}$: (global) stiffness matrix\n",
        "- $\\mathbf{u}$: unknown displacements (DOFs)\n",
        "- $\\mathbf{f}$: applied nodal loads\n",
        "\n",
        "Today: **how** we solve these efficiently and robustly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "part1-title",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Part 1 — Direct, Closed-Form Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1c43287",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Method 1: Direct Inversion\n",
        "> Mathematically valid to solve, via $\\mathbf{x}=\\mathbf{A}^{-1}\\mathbf{b}$.  Practically discouraged."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "inverse-why-not",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Why Direct Inversion is (Usually) a Bad Idea\n",
        "\n",
        "Computing $\\mathbf{A}^{-1}$ explicitly is typically:\n",
        "- **slower** than factorization + substitution (later in lecture)\n",
        "- **less accurate** (numerically)\n",
        "- **unnecessary** when all you want is $\\mathbf{x}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41d0b108",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### In NumPy\n",
        "\n",
        "We will use **NumPy** throughout the course.\n",
        "\n",
        "> Convention: solve $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ using  \n",
        "> `np.linalg.solve(A, b)`, not `inv(A) @ b`.\n",
        "\n",
        "```python\n",
        "x = np.linalg.solve(A, b)   # preferred: factorization + substitution\n",
        "# x = np.linalg.inv(A) @ b  # avoid: explicit inversion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "inv-vs-solve-slide",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Inversion vs. Solve (Same Math, Different Computation)\n",
        "\n",
        "Both compute a solution, but one is usually preferred:\n",
        "\n",
        "$$\n",
        "\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}\n",
        "\\quad \\text{(explicit inverse)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{x} = \\text{solve}(\\mathbf{A},\\mathbf{b})\n",
        "\\quad \\text{(factorization + substitution)}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "344424b2",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def compare_solve_vs_inverse(A, b, repeat=1):\n",
        "    # --- solve ---\n",
        "    t0 = time.perf_counter()\n",
        "    for _ in range(repeat):\n",
        "        x_solve = np.linalg.solve(A, b)\n",
        "    t_solve = (time.perf_counter() - t0) / repeat\n",
        "\n",
        "    # --- inverse ---\n",
        "    t0 = time.perf_counter()\n",
        "    for _ in range(repeat):\n",
        "        x_inv = np.linalg.inv(A) @ b\n",
        "    t_inv = (time.perf_counter() - t0) / repeat\n",
        "\n",
        "    return {\n",
        "        # \"x_solve\": x_solve,\n",
        "        # \"x_inv\": x_inv,\n",
        "        \"time_solve\": t_solve,\n",
        "        \"time_inv\": t_inv,\n",
        "        \"time_ratio_inv_over_solve\": t_inv / t_solve\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ca68011d",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time_solve: 2.7245829987805337e-06\n",
            "time_inv: 2.5919999752659352e-06\n",
            "time_ratio_inv_over_solve: 0.9513382328327161\n"
          ]
        }
      ],
      "source": [
        "A_small = np.array([[4.0, 2.0, 0.0],\n",
        "                    [2.0, 5.0, 1.0],\n",
        "                    [0.0, 1.0, 3.0]])\n",
        "\n",
        "b_small = np.array([2.0, 4.0, 6.0])\n",
        "\n",
        "result_small = compare_solve_vs_inverse(A_small, b_small, repeat=1000)\n",
        "\n",
        "for k, v in result_small.items():\n",
        "    print(f\"{k}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "10cd9de0",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time_solve: 5.580566357821226e-05\n",
            "time_inv: 0.00037019466011164087\n",
            "time_ratio_inv_over_solve: 6.633639605285025\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "n = 100\n",
        "\n",
        "# Symmetric positive definite matrix\n",
        "M = np.random.randn(n, n)\n",
        "A_large = M.T @ M + n * np.eye(n)\n",
        "b_large = np.random.randn(n)\n",
        "\n",
        "result_large = compare_solve_vs_inverse(A_large, b_large, repeat=3)\n",
        "\n",
        "for k, v in result_large.items():\n",
        "    print(f\"{k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cramer-slide",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Method 2: Cramer's Rule\n",
        "> Conceptually Useful, Practically Limited"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5abea8df",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "For a **small** system (especially $2\\times2$ or $3\\times3$), Cramer's rule is a clean closed form:\n",
        "\n",
        "$$\n",
        "x_i = \\frac{\\det(\\mathbf{A}_i)}{\\det(\\mathbf{A})}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{A}_i$ is $\\mathbf{A}$ with column $i$ replaced by $\\mathbf{b}$.\n",
        "\n",
        "For large $n$:\n",
        "- determinant costs scale poorly\n",
        "- numerically fragile\n",
        "\n",
        "> Cramer's rule is a teaching tool, not a production solver."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cramer-demo",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "det(A)= 5.000000000000001\n",
            "x (Cramer)= [ 2.8 -0.6]\n",
            "x (solve)=  [ 2.8 -0.6]\n",
            "residual ||Ax-b|| = 0.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.array([[2.0, 1.0],\n",
        "              [3.0, 4.0]])\n",
        "b = np.array([5.0, 6.0])\n",
        "\n",
        "detA = np.linalg.det(A)\n",
        "A1 = A.copy(); A1[:, 0] = b\n",
        "A2 = A.copy(); A2[:, 1] = b\n",
        "\n",
        "x_cramer = np.array([np.linalg.det(A1)/detA, np.linalg.det(A2)/detA])\n",
        "x_solve = np.linalg.solve(A, b)\n",
        "\n",
        "print('det(A)=', detA)\n",
        "print('x (Cramer)=', x_cramer)\n",
        "print('x (solve)= ', x_solve)\n",
        "print('residual ||Ax-b|| =', np.linalg.norm(A @ x_solve - b))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "part2-title",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Part 2 — Direct Elimination Methods\n",
        "\n",
        "> The idea: transform $\\mathbf{A}$ into a form that is easy to solve."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "elim-intro",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Big Picture: Elimination\n",
        "\n",
        "We use row operations to transform a linear system into a form that can be solved by substitution.\n",
        "\n",
        "Two closely related tasks:\n",
        "- Reduction of an augmented system $[\\mathbf{A}|\\mathbf{b}]$  \n",
        "  (conceptual, one-off solves)\n",
        "  - Method 1: Gaussian Elimination\n",
        "  - Method 2: Gauss–Jordan Elimination\n",
        "\n",
        "- Factorization of $\\mathbf{A}$  \n",
        "  (preferred for repeated solves)\n",
        "  - Method 3: LU Decomposition\n",
        "  - Method 4: Cholesky Factorization\n",
        "  - Method 5: $LDL^T$ Factorization\n",
        "  - Method 6: Thomas Method (special cases)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf72fcf9",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Method 1: Gaussian Elimination (GE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ge-slide",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "Goal\n",
        "- transform the system  \n",
        "  $$\n",
        "  \\mathbf{A}\\mathbf{x}=\\mathbf{b}\n",
        "  \\;\\longrightarrow\\;\n",
        "  \\mathbf{U}\\mathbf{x}=\\hat{\\mathbf{b}}\n",
        "  $$\n",
        "- where $\\mathbf{U}$ is upper triangular\n",
        "\n",
        "Important notes\n",
        "- GE is an elimination procedure, not a solver (IMPORTANT)\n",
        "- it systematically applies row operations to expose matrix structure\n",
        "- it is the foundation of:\n",
        "  - LU factorization  \n",
        "  - pivoted direct solvers  \n",
        "  - most practical linear algebra methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "291d79ec",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Gaussian Elimination — Procedure\n",
        "1. Choose the current pivot on the diagonal (the active diagonal entry used to eliminate entries below it).\n",
        "2. Eliminate all entries below the pivot using row operations.\n",
        "3. Apply the same row operations to the right-hand side, updating $\\mathbf{b}$.\n",
        "4. Move to the next column and repeat until $\\mathbf{A}\\rightarrow\\mathbf{U}$, $\\mathbf{b}\\rightarrow\\hat{\\mathbf{b}}$.\n",
        "\n",
        "$$\n",
        "\\mathbf{A}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & a_{14} \\\\\n",
        "a_{21} & a_{22} & a_{23} & a_{24} \\\\\n",
        "a_{31} & a_{32} & a_{33} & a_{34} \\\\\n",
        "a_{41} & a_{42} & a_{43} & a_{44}\n",
        "\\end{bmatrix}\n",
        "\\;\\xrightarrow{\\text{Gaussian Elimination}}\\;\n",
        "\\mathbf{U}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "u_{11} & u_{12} & u_{13} & u_{14} \\\\\n",
        "0      & u_{22} & u_{23} & u_{24} \\\\\n",
        "0      & 0      & u_{33} & u_{34} \\\\\n",
        "0      & 0      & 0      & u_{44}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Solving step\n",
        "- solve $\\mathbf{U}\\mathbf{x}=\\hat{\\mathbf{b}}$ by back-substitution\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f83612d3",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Method 2: Gauss–Jordan Elimination (GJ)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gj-slide",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "Goal\n",
        "- reduce the augmented system\n",
        "  $$\n",
        "  [\\mathbf{A}\\,|\\,\\mathbf{b}]\n",
        "  \\;\\longrightarrow\\;\n",
        "  [\\mathbf{I}\\,|\\,\\mathbf{x}]\n",
        "  $$\n",
        "\n",
        "Important notes\n",
        "- you operate directly on $\\mathbf{b}$ (so $\\mathbf{b}$ changes during elimination)\n",
        "- for repeated solves with different $\\mathbf{b}$, this is inefficient\n",
        "- can also apply GJ to computes the inverse of a matrix (when it exists)\n",
        "  $$\n",
        "  [\\mathbf{A}\\,|\\,\\mathbf{I}]\n",
        "  \\;\\longrightarrow\\;\n",
        "  [\\mathbf{I}\\,|\\,\\mathbf{A}^{-1}]\n",
        "  $$\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9af13dd1",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Gauss–Jordan Elimination — Procedure\n",
        "1. Form the augmented matrix $[\\mathbf{A}\\,|\\,\\mathbf{b}]$.\n",
        "2. Choose a pivot and scale the pivot row so the pivot entry becomes 1.\n",
        "3. Eliminate all entries below the pivot (like GE).\n",
        "4. Eliminate all entries above the pivot (this is the “Jordan” part).\n",
        "5. Repeat for the next column until $[\\mathbf{A}\\,|\\,\\mathbf{b}] \\rightarrow [\\mathbf{I}\\,|\\,\\mathbf{x}]$.\n",
        "\n",
        "$$\n",
        "\\left[\n",
        "\\begin{array}{c|c}\n",
        "\\mathbf{A} & \\mathbf{b}\n",
        "\\end{array}\n",
        "\\right]\n",
        "=\n",
        "\\left[\n",
        "\\begin{array}{cccc|c}\n",
        "a_{11} & a_{12} & a_{13} & a_{14} & b_1 \\\\\n",
        "a_{21} & a_{22} & a_{23} & a_{24} & b_2 \\\\\n",
        "a_{31} & a_{32} & a_{33} & a_{34} & b_3 \\\\\n",
        "a_{41} & a_{42} & a_{43} & a_{44} & b_4\n",
        "\\end{array}\n",
        "\\right]\n",
        "\\;\\xrightarrow{\\text{Gauss--Jordan Elimination}}\\;\n",
        "\\left[\n",
        "\\begin{array}{cccc|c}\n",
        "1 & 0 & 0 & 0 & x_1 \\\\\n",
        "0 & 1 & 0 & 0 & x_2 \\\\\n",
        "0 & 0 & 1 & 0 & x_3 \\\\\n",
        "0 & 0 & 0 & 1 & x_4\n",
        "\\end{array}\n",
        "\\right]\n",
        "=\n",
        "\\left[\n",
        "\\begin{array}{c|c}\n",
        "\\mathbf{I} & \\mathbf{x}\n",
        "\\end{array}\n",
        "\\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03d8d27d",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Gauss–Jordan — Worked Example (start)\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "5x_1 + 6x_2 - 3x_3 &= 66 \\\\\n",
        "9x_1 - x_2 + 2x_3 &= 8 \\\\\n",
        "8x_1 - 7x_2 + 4x_3 &= -39\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{A}=\n",
        "\\begin{bmatrix}\n",
        "5 & 6 & -3 \\\\\n",
        "9 & -1 & 2 \\\\\n",
        "8 & -7 & 4\n",
        "\\end{bmatrix},\n",
        "\\qquad\n",
        "\\mathbf{b}=\n",
        "\\begin{bmatrix}\n",
        "66 \\\\\n",
        "8 \\\\\n",
        "-39\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33f91483",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Step 0: Form augmented matrix\n",
        "\n",
        "$$\n",
        "\\left[\\mathbf{A}\\,|\\,\\mathbf{b}\\right]=\n",
        "\\left[\n",
        "\\begin{array}{ccc|c}\n",
        "5 & 6 & -3 & 66 \\\\\n",
        "9 & -1 & 2 & 8 \\\\\n",
        "8 & -7 & 4 & -39\n",
        "\\end{array}\n",
        "\\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b729bf35",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Step 1: pivot in column 1, make the pivot 1\n",
        "\n",
        "Row operation:\n",
        "- $R_1 \\leftarrow \\frac{1}{5}R_1$\n",
        "\n",
        "$$\n",
        "\\left[\n",
        "\\begin{array}{ccc|c}\n",
        "1 & 1.2 & -0.6 & 13.2 \\\\\n",
        "9 & -1 & 2 & 8 \\\\\n",
        "8 & -7 & 4 & -39\n",
        "\\end{array}\n",
        "\\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cba037d6",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Step 2: eliminate column 1 below the pivot\n",
        "\n",
        "Row operations:\n",
        "- $R_2 \\leftarrow R_2 - 9R_1$\n",
        "- $R_3 \\leftarrow R_3 - 8R_1$\n",
        "\n",
        "$$\n",
        "\\left[\n",
        "\\begin{array}{ccc|c}\n",
        "1 & 1.2 & -0.6 & 13.2 \\\\\n",
        "0 & -11.8 & 7.4 & -110.8 \\\\\n",
        "0 & -16.6 & 8.8 & -144.6\n",
        "\\end{array}\n",
        "\\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0a2011e",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Step 3: pivot in column 2, make the pivot 1\n",
        "\n",
        "Row operation:\n",
        "- $R_2 \\leftarrow \\frac{1}{-11.8}R_2$\n",
        "\n",
        "$$\n",
        "\\left[\n",
        "\\begin{array}{ccc|c}\n",
        "1 & 1.2 & -0.6 & 13.2 \\\\\n",
        "0 & 1 & -0.6271 & 9.39 \\\\\n",
        "0 & -16.6 & 8.8 & -144.6\n",
        "\\end{array}\n",
        "\\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "062e6660",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Step 4: Gauss–Jordan part, eliminate column 2 above and below the pivot\n",
        "\n",
        "Row operations:\n",
        "- $R_1 \\leftarrow R_1 - 1.2R_2$\n",
        "- $R_3 \\leftarrow R_3 + 16.6R_2$\n",
        "\n",
        "$$\n",
        "\\left[\n",
        "\\begin{array}{ccc|c}\n",
        "1 & 0 & 0.1525 & 1.932 \\\\\n",
        "0 & 1 & -0.6271 & 9.39 \\\\\n",
        "0 & 0 & -1.61 & 11.27\n",
        "\\end{array}\n",
        "\\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a0a0e90",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Step 5: pivot in column 3, make the pivot 1\n",
        "\n",
        "Row operation:\n",
        "- $R_3 \\leftarrow \\frac{1}{-1.61}R_3$\n",
        "\n",
        "$$\n",
        "\\left[\n",
        "\\begin{array}{ccc|c}\n",
        "1 & 0 & 0.1525 & 1.932 \\\\\n",
        "0 & 1 & -0.6271 & 9.39 \\\\\n",
        "0 & 0 & 1 & -7\n",
        "\\end{array}\n",
        "\\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8a2dc0f",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Step 6: eliminate column 3 above the pivot to reach $[\\mathbf{I}|\\mathbf{x}]$\n",
        "\n",
        "Row operations:\n",
        "- $R_1 \\leftarrow R_1 - 0.1525R_3$\n",
        "- $R_2 \\leftarrow R_2 + 0.6271R_3$\n",
        "\n",
        "$$\n",
        "\\left[\n",
        "\\begin{array}{ccc|c}\n",
        "1 & 0 & 0 & 3 \\\\\n",
        "0 & 1 & 0 & 5 \\\\\n",
        "0 & 0 & 1 & -7\n",
        "\\end{array}\n",
        "\\right]\n",
        "\\;=\\;\n",
        "\\left[\n",
        "\\begin{array}{c|c}\n",
        "\\mathbf{I} & \\mathbf{x}\n",
        "\\end{array}\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "So,\n",
        "$$\n",
        "\\mathbf{x}=\n",
        "\\begin{bmatrix}\n",
        "3\\\\\n",
        "5\\\\\n",
        "-7\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57167b82",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Gauss–Jordan Inverse (setup only)\n",
        "\n",
        "Given\n",
        "$$\n",
        "\\mathbf{A}=\n",
        "\\begin{bmatrix}\n",
        "4 & 1 & 2\\\\\n",
        "1 & 3 & 0\\\\\n",
        "2 & 0 & 5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Form the augmented matrix $[\\mathbf{A}\\,|\\,\\mathbf{I}]$\n",
        "$$\n",
        "\\left[\\mathbf{A}\\,|\\,\\mathbf{I}\\right]=\n",
        "\\left[\n",
        "\\begin{array}{ccc|ccc}\n",
        "4 & 1 & 2 & 1 & 0 & 0\\\\\n",
        "1 & 3 & 0 & 0 & 1 & 0\\\\\n",
        "2 & 0 & 5 & 0 & 0 & 1\n",
        "\\end{array}\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "Gauss–Jordan goal\n",
        "$$\n",
        "\\left[\\mathbf{A}\\,|\\,\\mathbf{I}\\right]\n",
        "\\;\\longrightarrow\\;\n",
        "\\left[\\mathbf{I}\\,|\\,\\mathbf{A}^{-1}\\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "gj-inverse-demo",
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def gauss_jordan_inverse(A):\n",
        "    \"\"\"Compute A^{-1} via Gauss–Jordan on [A | I]. Educational (not recommended for large systems).\"\"\"\n",
        "    A = A.astype(float).copy()\n",
        "    n = A.shape[0]\n",
        "    AI = np.hstack([A, np.eye(n)])\n",
        "\n",
        "    for k in range(n):\n",
        "        # Pivot (simple: choose max abs pivot in column k)\n",
        "        pivot_row = k + np.argmax(np.abs(AI[k:, k]))\n",
        "        if abs(AI[pivot_row, k]) < 1e-14:\n",
        "            raise ValueError('Matrix appears singular.')\n",
        "        if pivot_row != k:\n",
        "            AI[[k, pivot_row]] = AI[[pivot_row, k]]\n",
        "\n",
        "        # Normalize pivot row\n",
        "        pivot = AI[k, k]\n",
        "        AI[k, :] /= pivot\n",
        "\n",
        "        # Eliminate all other rows\n",
        "        for i in range(n):\n",
        "            if i == k:\n",
        "                continue\n",
        "            factor = AI[i, k]\n",
        "            AI[i, :] -= factor * AI[k, :]\n",
        "\n",
        "    invA = AI[:, n:]\n",
        "    return invA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2dfd88bc",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inv(A) via Gauss–Jordan=\n",
            " [[ 0.3488 -0.1163 -0.1395]\n",
            " [-0.1163  0.3721  0.0465]\n",
            " [-0.1395  0.0465  0.2558]]\n",
            "\n",
            "inv(A) via NumPy=\n",
            " [[ 0.3488 -0.1163 -0.1395]\n",
            " [-0.1163  0.3721  0.0465]\n",
            " [-0.1395  0.0465  0.2558]]\n",
            "\n",
            "check ||I - A inv(A)|| = 2.416688344895612e-16\n"
          ]
        }
      ],
      "source": [
        "A = np.array([[4.0, 1.0, 2.0],\n",
        "              [1.0, 3.0, 0.0],\n",
        "              [2.0, 0.0, 5.0]])\n",
        "\n",
        "invA_gj = gauss_jordan_inverse(A)\n",
        "invA_np = np.linalg.inv(A)\n",
        "\n",
        "print('inv(A) via Gauss–Jordan=\\n', invA_gj)\n",
        "print('\\ninv(A) via NumPy=\\n', invA_np)\n",
        "print('\\ncheck ||I - A inv(A)|| =', np.linalg.norm(np.eye(3) - A @ invA_gj))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84478eb8",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Decomposition Theorem\n",
        "\n",
        "> Quick aside before we cover methods 3-6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89bc733f",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "Elimination methods are based on the idea that a general square matrix\n",
        "can be factored into triangular (and sometimes diagonal) matrices.\n",
        "\n",
        "The decomposition theorem states:\n",
        "$$\n",
        "\\mathbf{A} = \\mathbf{L}\\mathbf{D}\\mathbf{U}\n",
        "$$\n",
        "\n",
        "where\n",
        "- $\\mathbf{L}$ is unit lower triangular\n",
        "- $\\mathbf{D}$ is diagonal\n",
        "- $\\mathbf{U}$ is unit upper triangular\n",
        "\n",
        "Example (3×3 case)\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} \\\\\n",
        "a_{21} & a_{22} & a_{23} \\\\\n",
        "a_{31} & a_{32} & a_{33}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 \\\\\n",
        "\\ell_{21} & 1 & 0 \\\\\n",
        "\\ell_{31} & \\ell_{32} & 1\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "d_1 & 0 & 0 \\\\\n",
        "0 & d_2 & 0 \\\\\n",
        "0 & 0 & d_3\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "1 & u_{12} & u_{13} \\\\\n",
        "0 & 1 & u_{23} \\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7bc7581",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Variants of Matrix Decomposition\n",
        "\n",
        "The specific elimination method depends on how the diagonal scaling\n",
        "is distributed among the triangular factors.\n",
        "\n",
        "Starting from\n",
        "$$\n",
        "\\mathbf{A} = \\mathbf{L}\\mathbf{D}\\mathbf{U},\n",
        "$$\n",
        "the diagonal matrix $\\mathbf{D}$ may be:\n",
        "- associated with $\\mathbf{L}$\n",
        "- associated with $\\mathbf{U}$\n",
        "- split between both\n",
        "\n",
        "As a result, a general square matrix may be factored as (generalization)\n",
        "$$\n",
        "\\mathbf{A} = \\mathbf{L}\\mathbf{U}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b54fb638",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "Important observations\n",
        "- the matrices labeled $\\mathbf{L}$ and $\\mathbf{U}$ here are **not unique**\n",
        "- they are **not necessarily the same** $\\mathbf{L}$ and $\\mathbf{U}$ appearing in $\\mathbf{A}=\\mathbf{L}\\mathbf{D}\\mathbf{U}$\n",
        "- their exact form depends on how the diagonal scaling is distributed\n",
        "- differences among elimination methods are largely differences in\n",
        "  how this factorization is constructed\n",
        "\n",
        "By regrouping factors, common variants are obtained:\n",
        "$$\n",
        "\\mathbf{A} = \\mathbf{L}\\mathbf{U}\n",
        "\\qquad\n",
        "\\mathbf{A} = \\mathbf{L}\\mathbf{D}\\mathbf{L}^T\n",
        "\\qquad\n",
        "\\mathbf{A} = \\mathbf{L}\\mathbf{L}^T\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4315e8d7",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Why is Matrix Decomposition Useful?\n",
        "\n",
        "Once a matrix has been factorized, solving $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ becomes a sequence of simple triangular solves.\n",
        "\n",
        "Using\n",
        "$$\n",
        "\\mathbf{A}=\\mathbf{L}\\mathbf{U},\n",
        "$$\n",
        "we rewrite the system as\n",
        "$$\n",
        "\\mathbf{L}\\mathbf{U}\\mathbf{x}=\\mathbf{b}.\n",
        "$$\n",
        "\n",
        "Introduce an auxiliary vector $\\hat{\\mathbf{b}}$:\n",
        "$$\n",
        "\\mathbf{U}\\mathbf{x}=\\hat{\\mathbf{b}}\n",
        "$$\n",
        "\n",
        "Substitute into $\\mathbf{L}\\mathbf{U}\\mathbf{x}=\\mathbf{b}$:\n",
        "$$\n",
        "\\mathbf{L}\\hat{\\mathbf{b}}=\\mathbf{b}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3388d183",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Solving: First Forward Substitution\n",
        "\n",
        "Since $\\mathbf{L}$ is a known lower triangular matrix and $\\mathbf{b}$ is given,\n",
        "the auxiliary vector $\\hat{\\mathbf{b}}$ can be computed element by element\n",
        "using forward substitution.\n",
        "\n",
        "$$\n",
        "\\mathbf{L}\\hat{\\mathbf{b}}=\\mathbf{b}.\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "\\ell_{11} & 0           & 0           & \\cdots & 0 \\\\\n",
        "\\ell_{21} & \\ell_{22}   & 0           & \\cdots & 0 \\\\\n",
        "\\ell_{31} & \\ell_{32}   & \\ell_{33}   & \\cdots & 0 \\\\\n",
        "\\vdots    & \\vdots      & \\vdots      & \\ddots & 0 \\\\\n",
        "\\ell_{n1} & \\ell_{n2}   & \\ell_{n3}   & \\cdots & \\ell_{nn}\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "\\hat b_1 \\\\\n",
        "\\hat b_2 \\\\\n",
        "\\hat b_3 \\\\\n",
        "\\vdots   \\\\\n",
        "\\hat b_n\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "b_1 \\\\\n",
        "b_2 \\\\\n",
        "b_3 \\\\\n",
        "\\vdots \\\\\n",
        "b_n\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Forward substitution\n",
        "$$\n",
        "\\hat b_i\n",
        "=\n",
        "\\frac{\n",
        "b_i - \\sum_{j=1}^{i-1} \\ell_{ij}\\,\\hat b_j\n",
        "}{\n",
        "\\ell_{ii}\n",
        "},\n",
        "\\qquad\n",
        "i = 1,\\ldots,n\n",
        "$$\n",
        "\n",
        "This allows $\\hat{\\mathbf{b}}$ to be computed sequentially\n",
        "from top to bottom.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff65959f",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Solving: Then Backward Substitution\n",
        "\n",
        "Once $\\hat{\\mathbf{b}}$ is known, and $\\mathbf{U}$ is a known upper triangular\n",
        "matrix, the solution vector $\\mathbf{x}$ is obtained element by element\n",
        "using back substitution.\n",
        "\n",
        "$$\n",
        "\\mathbf{U}\\mathbf{x}=\\hat{\\mathbf{b}}.\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "u_{11} & u_{12} & u_{13} & \\cdots & u_{1n} \\\\\n",
        "0      & u_{22} & u_{23} & \\cdots & u_{2n} \\\\\n",
        "0      & 0      & u_{33} & \\cdots & u_{3n} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0      & 0      & 0      & \\cdots & u_{nn}\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "x_1 \\\\\n",
        "x_2 \\\\\n",
        "x_3 \\\\\n",
        "\\vdots \\\\\n",
        "x_n\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\hat b_1 \\\\\n",
        "\\hat b_2 \\\\\n",
        "\\hat b_3 \\\\\n",
        "\\vdots \\\\\n",
        "\\hat b_n\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Back substitution\n",
        "$$\n",
        "x_i\n",
        "=\n",
        "\\frac{\n",
        "\\hat b_i - \\sum_{j=i+1}^{n} u_{ij}\\,x_j\n",
        "}{\n",
        "u_{ii}\n",
        "},\n",
        "\\qquad\n",
        "i = n,\\ldots,1\n",
        "$$\n",
        "\n",
        "This allows $\\mathbf{x}$ to be computed sequentially\n",
        "from bottom to top."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed91ffa3",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Why Factorization Helps for Repeated Solves\n",
        "\n",
        "If the right-hand side changes ($\\mathbf{b}$ changes), we do not need to refactor $\\mathbf{A}$.\n",
        "\n",
        "- $\\mathbf{A}$ is factorized once  \n",
        "- each new $\\mathbf{b}$ only requires:\n",
        "  - forward substitution\n",
        "  - back substitution\n",
        "\n",
        "In contrast, Gauss–Jordan operates on the augmented system $[\\mathbf{A}\\,|\\,\\mathbf{b}]$,\n",
        "so a new $\\mathbf{b}$ requires repeating the full elimination.\n",
        "\n",
        "Example (operation counts from the code)\n",
        "- $n=30$, number of right-hand sides = 30\n",
        "- LU total work (factor once + 30 solves): 74,215 ops\n",
        "- Gauss–Jordan total work (redo elimination 30 times): 876,150 ops\n",
        "- ratio: Gauss–Jordan / LU $\\approx 11.81\\times$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7acad6a",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### A Unified View\n",
        "\n",
        "All of the following methods share the same structure:\n",
        "- factorize the matrix once  $\\mathcal{O}(n^3)$\n",
        "- for each new $\\mathbf{b}$:\n",
        "  - forward substitution  $\\mathcal{O}(n^2)$\n",
        "  - back substitution  $\\mathcal{O}(n^2)$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83562717",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Method 3: LU Decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6a83c32",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "If $\\mathbf{A}$ is square:\n",
        "$$\n",
        "\\mathbf{A}=\\mathbf{L}\\mathbf{U}\n",
        "$$\n",
        "\n",
        "Important notes\n",
        "- $\\mathbf{L}$ and $\\mathbf{U}$ are **not unique**\n",
        "\n",
        "What “not unique” means\n",
        "- there are **many valid pairs** $(\\mathbf{L},\\mathbf{U})$ whose product equals $\\mathbf{A}$\n",
        "- different elimination choices (scaling, ordering, pivoting) produce different factors\n",
        "- the matrices labeled $\\mathbf{L}$ and $\\mathbf{U}$ represent **roles**, not a single fixed decomposition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82f0c799",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "Consider the matrix\n",
        "$$\n",
        "\\mathbf{A}=\n",
        "\\begin{bmatrix}\n",
        "2 & 2 & 0\\\\\n",
        "2 & 4 & 2\\\\\n",
        "0 & 2 & 2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "One valid LU factorization:\n",
        "$$\n",
        "\\mathbf{A}=\\mathbf{L}_1\\mathbf{U}_1,\n",
        "\\qquad\n",
        "\\mathbf{L}_1=\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0\\\\\n",
        "1 & 1 & 0\\\\\n",
        "0 & 1 & 1\n",
        "\\end{bmatrix},\n",
        "\\quad\n",
        "\\mathbf{U}_1=\n",
        "\\begin{bmatrix}\n",
        "2 & 2 & 0\\\\\n",
        "0 & 2 & 2\\\\\n",
        "0 & 0 & 2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Another valid LU factorization (rescaling the diagonal):\n",
        "$$\n",
        "\\mathbf{A}=\\mathbf{L}_2\\mathbf{U}_2,\n",
        "\\qquad\n",
        "\\mathbf{L}_2=\n",
        "\\begin{bmatrix}\n",
        "2 & 0 & 0\\\\\n",
        "2 & 2 & 0\\\\\n",
        "0 & 2 & 2\n",
        "\\end{bmatrix},\n",
        "\\quad\n",
        "\\mathbf{U}_2=\n",
        "\\begin{bmatrix}\n",
        "1 & 1 & 0\\\\\n",
        "0 & 1 & 1\\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Both satisfy\n",
        "$$\n",
        "\\mathbf{L}_1\\mathbf{U}_1\n",
        "=\n",
        "\\mathbf{L}_2\\mathbf{U}_2\n",
        "=\n",
        "\\mathbf{A}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d79bbd1",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### LU Decomposition — Example Factorization\n",
        "\n",
        "Start with a simple 3×3 matrix\n",
        "$$\n",
        "\\mathbf{A}=\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13}\\\\\n",
        "a_{21} & a_{22} & a_{23}\\\\\n",
        "a_{31} & a_{32} & a_{33}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Goal\n",
        "$$\n",
        "\\mathbf{A}=\\mathbf{L}\\mathbf{U}\n",
        "$$\n",
        "\n",
        "We create $\\mathbf{U}$ by doing Gaussian Elimination on $\\mathbf{A}$, and we build $\\mathbf{L}$ by recording the elimination multipliers needed to form $\\mathbf{U}$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4ea6f74",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "Why this works\n",
        "- each elimination step subtracts a multiple of one row from another\n",
        "- these row operations can be written as multiplication by a lower triangular matrix\n",
        "- the multipliers used to eliminate entries below the diagonal are exactly the\n",
        "  subdiagonal entries of $\\mathbf{L}$\n",
        "\n",
        "As a result\n",
        "- Gaussian Elimination transforms $\\mathbf{A}$ into $\\mathbf{U}$\n",
        "- the accumulated elimination operations form $\\mathbf{L}$\n",
        "- together, they satisfy\n",
        "  $$\n",
        "  \\mathbf{A}=\\mathbf{L}\\mathbf{U}\n",
        "  $$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36bd8510",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Step 1: Eliminate below the first pivot\n",
        "\n",
        "First pivot\n",
        "$$\n",
        "u_{11}=a_{11}\n",
        "$$\n",
        "\n",
        "Elimination multipliers (what gets stored in $\\mathbf{L}$)\n",
        "$$\n",
        "\\ell_{21}=\\frac{a_{21}}{a_{11}},\n",
        "\\qquad\n",
        "\\ell_{31}=\\frac{a_{31}}{a_{11}}\n",
        "$$\n",
        "\n",
        "Row operations (update the matrix to start forming $\\mathbf{U}$)\n",
        "$$\n",
        "R_2 \\leftarrow R_2-\\ell_{21}R_1,\n",
        "\\qquad\n",
        "R_3 \\leftarrow R_3-\\ell_{31}R_1\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de01e6e9",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Step 1: What $\\mathbf{L}$ records\n",
        "\n",
        "The multipliers used to eliminate entries become entries in $\\mathbf{L}$.\n",
        "\n",
        "After the first elimination stage:\n",
        "$$\n",
        "\\mathbf{L}=\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0\\\\\n",
        "\\ell_{21} & 1 & 0\\\\\n",
        "\\ell_{31} & 0 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "At this point, $\\mathbf{U}$ is the partially eliminated matrix:\n",
        "$$\n",
        "\\mathbf{U}=\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13}\\\\\n",
        "0      & a'_{22}& a'_{23}\\\\\n",
        "0      & a'_{32}& a'_{33}\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07d071bd",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Step 2: Eliminate below the second pivot\n",
        "\n",
        "Second pivot\n",
        "$$\n",
        "u_{22}=a'_{22}\n",
        "$$\n",
        "\n",
        "Multiplier for the entry below it (store in $\\mathbf{L}$)\n",
        "$$\n",
        "\\ell_{32}=\\frac{a'_{32}}{a'_{22}}\n",
        "$$\n",
        "\n",
        "Row operation (finish forming $\\mathbf{U}$)\n",
        "$$\n",
        "R_3 \\leftarrow R_3-\\ell_{32}R_2\n",
        "$$\n",
        "\n",
        "Now the matrix is upper triangular:\n",
        "$$\n",
        "\\mathbf{U}=\n",
        "\\begin{bmatrix}\n",
        "u_{11} & u_{12} & u_{13}\\\\\n",
        "0      & u_{22} & u_{23}\\\\\n",
        "0      & 0      & u_{33}\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4caa0fbe",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Step 2: Final $\\mathbf{L}$ and $\\mathbf{U}$\n",
        "\n",
        "Collect all stored multipliers:\n",
        "$$\n",
        "\\mathbf{L}=\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0\\\\\n",
        "\\ell_{21} & 1 & 0\\\\\n",
        "\\ell_{31} & \\ell_{32} & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "And the eliminated matrix is $\\mathbf{U}$:\n",
        "$$\n",
        "\\mathbf{U}=\n",
        "\\begin{bmatrix}\n",
        "u_{11} & u_{12} & u_{13}\\\\\n",
        "0      & u_{22} & u_{23}\\\\\n",
        "0      & 0      & u_{33}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Interpretation\n",
        "- $\\mathbf{U}$ is what you get after Gaussian Elimination\n",
        "- $\\mathbf{L}$ stores the multipliers that created the zeros"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6ee0c07",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Method 4: Cholesky Factorization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f227d71",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "If $\\mathbf{A}$ is **symmetric positive definite (SPD)**:\n",
        "$$\n",
        "\\mathbf{A}=\\mathbf{L}\\mathbf{U}=\\mathbf{L}\\mathbf{L}^T\n",
        "$$\n",
        "\n",
        "Important notes\n",
        "- symmetry implies the same elimination occurs above and below the diagonal\n",
        "- the upper triangular factor satisfies\n",
        "  $$\n",
        "  \\mathbf{U}=\\mathbf{L}^T\n",
        "  $$\n",
        "- minimal storage and fastest solves\n",
        "- This is the fastest standard dense factorization for SPD systems.\n",
        "\n",
        "In structural analysis:\n",
        "- many stiffness matrices are SPD after applying boundary conditions (for typical linear elastic problems)\n",
        "\n",
        "<!-- A symmetric matrix $\\mathbf{A}$ is **positive definite** if\n",
        "$$\n",
        "\\mathbf{x}^T \\mathbf{A} \\mathbf{x} > 0\n",
        "\\quad\n",
        "\\text{for all nonzero } \\mathbf{x}.\n",
        "$$ -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e27025ce",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Constructing $L$ \n",
        "\n",
        "For an SPD matrix $\\mathbf{A}$,\n",
        "the Cholesky factorization\n",
        "can be constructed column by column using the following recurrence relation\n",
        "\n",
        "Diagonal entries\n",
        "$$\n",
        "\\ell_{ii}\n",
        "=\n",
        "\\sqrt{\n",
        "a_{ii}\n",
        "-\n",
        "\\sum_{k=1}^{i-1}\n",
        "\\ell_{ik}^2\n",
        "}\n",
        "$$\n",
        "\n",
        "Off-diagonal entries\n",
        "$$\n",
        "\\ell_{ji}\n",
        "=\n",
        "\\frac{\n",
        "a_{ji}\n",
        "-\n",
        "\\sum_{k=1}^{i-1}\n",
        "\\ell_{jk}\\,\\ell_{ik}\n",
        "}{\n",
        "\\ell_{ii}\n",
        "},\n",
        "\\qquad\n",
        "j=i+1,\\ldots,n\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef346755",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### But what if negative number in the root?\n",
        "\n",
        "- this implies $\\mathbf{A}$ is **not positive definite**\n",
        "- the Cholesky factorization **fails**\n",
        "- mathematically: $\\mathbf{A}$ is indefinite\n",
        "\n",
        "Structural interpretation\n",
        "- in structural analysis, $\\mathbf{A}=\\mathbf{K}_{ff}$ (stiffness matrix)\n",
        "- a negative value under the square root indicates **instability**\n",
        "- the structure cannot resist certain deformation modes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96a301d2",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Positive Definiteness — How to tell? (Engineering Intuition)\n",
        "\n",
        "Given\n",
        "$$\n",
        "\\mathbf{A} =\n",
        "\\begin{bmatrix}\n",
        "4 & 1 & 0 \\\\\n",
        "1 & 3 & 1 \\\\\n",
        "0 & 1 & 2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "Observations:\n",
        "\n",
        "- $\\mathbf{A}$ is **symmetric**\n",
        "- All diagonal entries are **positive**\n",
        "- Each diagonal term is **larger than the sum of off-diagonal terms in its row**\n",
        "\n",
        "Structural interpretation\n",
        "- Each DOF has more **self-stiffness** than coupling to neighboring DOFs  \n",
        "- Any nonzero displacement stores **positive strain energy**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "579c0976",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Cholesky Example (Setup)\n",
        "\n",
        "Compute the Cholesky factorization\n",
        "$$\n",
        "\\mathbf{A}=\\mathbf{L}\\mathbf{L}^T\n",
        "$$\n",
        "for\n",
        "$$\n",
        "\\mathbf{A}=\n",
        "\\begin{bmatrix}\n",
        "100 & 30 & 20 & 10 \\\\\n",
        "30 & 178 & 201 & -36 \\\\\n",
        "20 & 201 & 485 & 21 \\\\\n",
        "10 & -36 & 21 & 350\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "We compute $\\mathbf{L}$ column by column.\n",
        "\n",
        "Recall: $\\mathbf{L}$ stores the elimination coefficients used to zero entries below the diagonal in calculating $\\mathbf{U}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ed25261",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Column 1\n",
        "\n",
        "Diagonal entry\n",
        "$$\n",
        "\\ell_{11}=\\sqrt{a_{11}}=\\sqrt{100}=10\n",
        "$$\n",
        "\n",
        "Below-diagonal entries\n",
        "$$\n",
        "\\ell_{21}=\\frac{a_{21}}{\\ell_{11}}=\\frac{30}{10}=3,\n",
        "\\qquad\n",
        "\\ell_{31}=\\frac{a_{31}}{\\ell_{11}}=\\frac{20}{10}=2,\n",
        "\\qquad\n",
        "\\ell_{41}=\\frac{a_{41}}{\\ell_{11}}=\\frac{10}{10}=1\n",
        "$$\n",
        "\n",
        "After column 1\n",
        "$$\n",
        "\\mathbf{L}=\n",
        "\\begin{bmatrix}\n",
        "10 & 0 & 0 & 0\\\\\n",
        "3  & 0 & 0 & 0\\\\\n",
        "2  & 0 & 0 & 0\\\\\n",
        "1  & 0 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2052ddb",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Column 2\n",
        "\n",
        "Diagonal entry\n",
        "$$\n",
        "\\ell_{22}=\\sqrt{a_{22}-\\ell_{21}^2}\n",
        "=\\sqrt{178-3^2}\n",
        "=\\sqrt{169}=13\n",
        "$$\n",
        "\n",
        "Below-diagonal entries\n",
        "$$\n",
        "\\ell_{32}=\\frac{a_{32}-\\ell_{31}\\ell_{21}}{\\ell_{22}}\n",
        "=\\frac{201-(2\\cdot 3)}{13}\n",
        "=\\frac{195}{13}=15\n",
        "$$\n",
        "$$\n",
        "\\ell_{42}=\\frac{a_{42}-\\ell_{41}\\ell_{21}}{\\ell_{22}}\n",
        "=\\frac{-36-(1\\cdot 3)}{13}\n",
        "=\\frac{-39}{13}=-3\n",
        "$$\n",
        "\n",
        "After column 2\n",
        "$$\n",
        "\\mathbf{L}=\n",
        "\\begin{bmatrix}\n",
        "10 & 0  & 0 & 0\\\\\n",
        "3  & 13 & 0 & 0\\\\\n",
        "2  & 15 & 0 & 0\\\\\n",
        "1  & -3 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d67eac13",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Column 3\n",
        "\n",
        "Diagonal entry\n",
        "$$\n",
        "\\ell_{33}=\\sqrt{a_{33}-\\ell_{31}^2-\\ell_{32}^2}\n",
        "=\\sqrt{485-2^2-15^2}\n",
        "=\\sqrt{485-4-225}\n",
        "=\\sqrt{256}=16\n",
        "$$\n",
        "\n",
        "Below-diagonal entry\n",
        "$$\n",
        "\\ell_{43}=\\frac{a_{43}-\\ell_{41}\\ell_{31}-\\ell_{42}\\ell_{32}}{\\ell_{33}}\n",
        "=\\frac{21-(1\\cdot 2)-((-3)\\cdot 15)}{16}\n",
        "=\\frac{21-2+45}{16}\n",
        "=\\frac{64}{16}=4\n",
        "$$\n",
        "\n",
        "After column 3\n",
        "$$\n",
        "\\mathbf{L}=\n",
        "\\begin{bmatrix}\n",
        "10 & 0  & 0  & 0\\\\\n",
        "3  & 13 & 0  & 0\\\\\n",
        "2  & 15 & 16 & 0\\\\\n",
        "1  & -3 & 4  & 0\n",
        "\\end{bmatrix}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44e5778f",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Column 4\n",
        "\n",
        "Diagonal entry\n",
        "$$\n",
        "\\ell_{44}=\\sqrt{a_{44}-\\ell_{41}^2-\\ell_{42}^2-\\ell_{43}^2}\n",
        "=\\sqrt{350-1^2-(-3)^2-4^2}\n",
        "=\\sqrt{350-1-9-16}\n",
        "=\\sqrt{324}=18\n",
        "$$\n",
        "\n",
        "Final factor\n",
        "$$\n",
        "\\mathbf{L}=\n",
        "\\begin{bmatrix}\n",
        "10 & 0  & 0  & 0\\\\\n",
        "3  & 13 & 0  & 0\\\\\n",
        "2  & 15 & 16 & 0\\\\\n",
        "1  & -3 & 4  & 18\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dd351d5",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Check\n",
        "\n",
        "Compute\n",
        "$$\n",
        "\\mathbf{L}\\mathbf{L}^T\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "10 & 0  & 0  & 0\\\\\n",
        "3  & 13 & 0  & 0\\\\\n",
        "2  & 15 & 16 & 0\\\\\n",
        "1  & -3 & 4  & 18\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "10 & 3  & 2  & 1\\\\\n",
        "0  & 13 & 15 & -3\\\\\n",
        "0  & 0  & 16 & 4\\\\\n",
        "0  & 0  & 0  & 18\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "100 & 30 & 20 & 10 \\\\\n",
        "30 & 178 & 201 & -36 \\\\\n",
        "20 & 201 & 485 & 21 \\\\\n",
        "10 & -36 & 21 & 350\n",
        "\\end{bmatrix}\n",
        "=\\mathbf{A}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e33c376",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Method 5: LDL$^T$ Factorization\n",
        "\n",
        ">Also known as root-free Cholesky"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d4e8970",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "If $\\mathbf{A}$ is symmetric (does not need to be definite):\n",
        "$$\n",
        "\\mathbf{A}=\\mathbf{L}\\mathbf{D}\\mathbf{U}=\\mathbf{L}\\mathbf{D}\\mathbf{L}^T\n",
        "$$\n",
        "\n",
        "Important notes\n",
        "- symmetry implies the same elimination occurs above and below the diagonal\n",
        "- the upper triangular factor satisfies\n",
        "  $$\n",
        "  \\mathbf{U}=\\mathbf{L}^T\n",
        "  $$\n",
        "- $\\mathbf{L}$ is unit lower triangular\n",
        "- $\\mathbf{D}$ is diagonal and retains the scaling\n",
        "- improved numberical stability"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "chol-slide",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Constructing $L$ and $D$\n",
        "\n",
        "For a symmetric matrix $\\mathbf{A}$, the $LDL^T$ factorization\n",
        "$$\n",
        "\\mathbf{A}=\\mathbf{L}\\mathbf{D}\\mathbf{L}^T\n",
        "$$\n",
        "can be constructed column by column using the following recurrence relations.\n",
        "\n",
        "Diagonal entries of $\\mathbf{D}$ (no square root required!)\n",
        "$$\n",
        "d_{ii}\n",
        "=\n",
        "a_{ii}\n",
        "-\n",
        "\\sum_{k=1}^{i-1}\n",
        "d_{kk}\\,\\ell_{ik}^2\n",
        "$$\n",
        "\n",
        "Diagonal entries of $\\mathbf{L}$\n",
        "$$\n",
        "\\ell_{ii}=1\n",
        "$$\n",
        "\n",
        "Off-diagonal entries of $\\mathbf{L}$\n",
        "$$\n",
        "\\ell_{ji}\n",
        "=\n",
        "\\frac{\n",
        "a_{ji}\n",
        "-\n",
        "\\sum_{k=1}^{i-1}\n",
        "d_{kk}\\,\\ell_{jk}\\,\\ell_{ik}\n",
        "}{\n",
        "d_{ii}\n",
        "},\n",
        "\\qquad\n",
        "j=i+1,\\ldots,n\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9f23d37",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Method 6: Thomas Method\n",
        "\n",
        "> Only in very special case, Tridiagonal Matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "thomas-slide",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "If $\\mathbf{A}$ is **tridiagonal** (bandwidth = 3):\n",
        "- only $a_{i,i-1}$, $a_{i,i}$, $a_{i,i+1}$ are nonzero\n",
        "\n",
        "$$\n",
        "\\mathbf{A}=\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & 0      & 0      & \\cdots & 0 \\\\\n",
        "a_{21} & a_{22} & a_{23} & 0      & \\cdots & 0 \\\\\n",
        "0      & a_{32} & a_{33} & a_{34} & \\cdots & 0 \\\\\n",
        "\\vdots & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n",
        "0      & \\cdots & 0      & a_{n,n-1} & a_{nn-1} & a_{nn} \\\\\n",
        "0      & \\cdots & 0      & 0      & a_{n,n-1} & a_{nn}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Thomas algorithm is a specialized LU that runs in **$\\mathcal{O}(n)$** time.\n",
        "\n",
        "This appears often in:\n",
        "- 1D finite difference / finite element problems\n",
        "- beam/rod discretizations with local coupling (each DOF interacts only with its neighbors)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55fef267",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Final Remark (All Direct Methods)\n",
        "\n",
        "Once the matrix is factorized, the factors do not change; only $\\mathbf{b}$ does.\n",
        "\n",
        "**LU**\n",
        "- $\\mathbf{L} \\hat{\\mathbf{b}}=\\mathbf{b}$   \n",
        "- $\\mathbf{U} \\mathbf{x}=\\hat{\\mathbf{b}}$  \n",
        "\n",
        "**Cholesky**\n",
        "- $\\mathbf{L}\\hat{\\mathbf{b}}=\\mathbf{b}$  \n",
        "- $\\mathbf{L}^T\\mathbf{x}=\\hat{\\mathbf{b}}$  \n",
        "\n",
        "**LDL$^T$**\n",
        "- $\\mathbf{L}\\hat{\\mathbf{b}}=\\mathbf{b}$  \n",
        "- $\\mathbf{D}\\mathbf{L}^T\\mathbf{x}=\\hat{\\mathbf{b}}$  \n",
        "\n",
        "Same forward/backward substitution structure for all methods."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "part3-title",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Part 3 — Iterative Methods\n",
        "\n",
        "> Replace one big solve with a sequence of cheap updates: $\\mathbf{x}^{(k)} \\to \\mathbf{x}^{(k+1)}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iter-why",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Why Iterative Solvers?\n",
        "\n",
        "Iterative methods become attractive when:\n",
        "- $\\mathbf{A}$ is **very large** and sparse\n",
        "- you only need an approximate solution (within tolerance)\n",
        "- factorization would cause too much fill-in (memory)\n",
        "\n",
        "But:\n",
        "- convergence is not guaranteed for all matrices\n",
        "- performance depends strongly on conditioning and preconditioning\n",
        "- can take longer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04b46695",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Method 1: Point Jacobi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jacobi-slide",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "The Point Jacobi method solves\n",
        "$$\n",
        "\\mathbf{A}\\mathbf{x}=\\mathbf{b}\n",
        "$$\n",
        "by starting with an initial guess $\\mathbf{x}^{(0)}$ and repeatedly updating each\n",
        "unknown using values from the **previous** iteration.\n",
        "\n",
        "Key idea\n",
        "- compute a new vector $\\mathbf{x}^{(m)}$ using only $\\mathbf{x}^{(m-1)}$\n",
        "- all components are updated “in parallel” (no immediate reuse)\n",
        "\n",
        "Requirement\n",
        "- diagonal entries must be nonzero: $a_{ii}\\neq 0$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "587015a1",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Component Form (3×3 Example)\n",
        "\n",
        "System\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13}\\\\\n",
        "a_{21} & a_{22} & a_{23}\\\\\n",
        "a_{31} & a_{32} & a_{33}\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "x_1\\\\x_2\\\\x_3\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "b_1\\\\b_2\\\\b_3\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Update formula (component form)\n",
        "$$\n",
        "x_i^{(m)}\n",
        "=\n",
        "\\frac{\n",
        "b_i-\\sum_{j\\ne i} a_{ij}\\,x_j^{(m-1)}\n",
        "}{\n",
        "a_{ii}\n",
        "},\n",
        "\\qquad\n",
        "i=1,\\ldots,n\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "730f5568",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Equation Form (3×3 Example)\n",
        "\n",
        "Start from the linear system at iteration $m$:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "a_{11}x_1^{(m-1)} + a_{12}x_2^{(m-1)} + a_{13}x_3^{(m-1)} &= b_1 \\\\\n",
        "a_{21}x_1^{(m-1)} + a_{22}x_2^{(m-1)} + a_{23}x_3^{(m-1)} &= b_2 \\\\\n",
        "a_{31}x_1^{(m-1)} + a_{32}x_2^{(m-1)} + a_{33}x_3^{(m-1)} &= b_3\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Jacobi updates (use only previous iteration values)\n",
        "$$\n",
        "\\boxed{\n",
        "x_1^{(m)}\n",
        "}\n",
        "=\n",
        "\\frac{b_1-a_{12}x_2^{(m-1)}-a_{13}x_3^{(m-1)}}{a_{11}}\n",
        "$$\n",
        "$$\n",
        "\\boxed{\n",
        "x_2^{(m)}\n",
        "}\n",
        "=\n",
        "\\frac{b_2-a_{21}x_1^{(m-1)}-a_{23}x_3^{(m-1)}}{a_{22}}\n",
        "$$\n",
        "$$\n",
        "\\boxed{\n",
        "x_3^{(m)}\n",
        "}\n",
        "=\n",
        "\\frac{b_3-a_{31}x_1^{(m-1)}-a_{32}x_2^{(m-1)}}{a_{33}}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c937059a",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "One iteration step\n",
        "- start with $\\mathbf{x}^{(m-1)}$\n",
        "- compute $x_1^{(m)},x_2^{(m)},x_3^{(m)}$\n",
        "- collect into the new vector $\\mathbf{x}^{(m)}$\n",
        "- repeat iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbc5699c",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Method 2: Point Gauss–Seidel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gs-slide",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "The Point Gauss–Seidel method solves\n",
        "$$\n",
        "\\mathbf{A}\\mathbf{x}=\\mathbf{b}\n",
        "$$\n",
        "by starting with an initial guess $\\mathbf{x}^{(0)}$ and repeatedly updating each\n",
        "unknown using the **newest available values**.\n",
        "\n",
        "Key idea\n",
        "- compute $\\mathbf{x}^{(m)}$ by immediately reusing updated components\n",
        "- values $x_1^{(m)},\\ldots,x_{i-1}^{(m)}$ are reused when computing $x_i^{(m)}$\n",
        "- updates are **sequential**, not parallel\n",
        "\n",
        "Requirement\n",
        "- diagonal entries must be nonzero: $a_{ii}\\neq 0$\n",
        "\n",
        "Notes\n",
        "- often converges faster than Point Jacobi\n",
        "- less parallel, but more efficient per iteration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dccf3c5c",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Component Form (3×3 Example)\n",
        "\n",
        "System\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13}\\\\\n",
        "a_{21} & a_{22} & a_{23}\\\\\n",
        "a_{31} & a_{32} & a_{33}\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "x_1\\\\x_2\\\\x_3\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "b_1\\\\b_2\\\\b_3\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Update formula (component form)\n",
        "$$\n",
        "x_i^{(m)}\n",
        "=\n",
        "\\frac{\n",
        "b_i\n",
        "-\n",
        "\\sum_{j<i} a_{ij}\\,x_j^{(m)}\n",
        "-\n",
        "\\sum_{j>i} a_{ij}\\,x_j^{(m-1)}\n",
        "}{\n",
        "a_{ii}\n",
        "},\n",
        "\\qquad\n",
        "i=1,\\ldots,n\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd3caa60",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Equation Form (3×3 Example)\n",
        "\n",
        "Start from the linear system at iteration $m$:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "a_{11}x_1^{(m)} + a_{12}x_2^{(m-1)} + a_{13}x_3^{(m-1)} &= b_1 \\\\\n",
        "a_{21}x_1^{(m)} + a_{22}x_2^{(m)} + a_{23}x_3^{(m-1)} &= b_2 \\\\\n",
        "a_{31}x_1^{(m)} + a_{32}x_2^{(m)} + a_{33}x_3^{(m)} &= b_3\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Gauss–Seidel updates  \n",
        "(reuse newest available values immediately)\n",
        "\n",
        "$$\n",
        "\\boxed{x_1^{(m)}}\n",
        "=\n",
        "\\frac{b_1-a_{12}x_2^{(m-1)}-a_{13}x_3^{(m-1)}}{a_{11}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\boxed{x_2^{(m)}}\n",
        "=\n",
        "\\frac{b_2-a_{21}\\boxed{x_1^{(m)}}-a_{23}x_3^{(m-1)}}{a_{22}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\boxed{x_3^{(m)}}\n",
        "=\n",
        "\\frac{b_3-a_{31}\\boxed{x_1^{(m)}}-a_{32}\\boxed{x_2^{(m)}}}{a_{33}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a30a676",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "One iteration step\n",
        "- start with $\\mathbf{x}^{(m-1)}$\n",
        "- compute $x_1^{(m)}$ and reuse it immediately\n",
        "- compute $x_2^{(m)}$ using updated $x_1^{(m)}$\n",
        "- compute $x_3^{(m)}$ using updated $x_1^{(m)},x_2^{(m)}$\n",
        "- collect into the new vector $\\mathbf{x}^{(m)}$\n",
        "- repeat iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d290db9",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Jacobi vs. Gauss–Seidel (Same Equations, Different Reuse)\n",
        "\n",
        "$$\n",
        "\\begin{array}{c c}\n",
        "\\textbf{Point Jacobi (parallel update)} & \\textbf{Point Gauss-Seidel (sequential reuse)} \\\\[6pt]\n",
        "\n",
        "\\boxed{x_1^{(m)}}=\n",
        "\\frac{b_1-a_{12}x_2^{(m-1)}-a_{13}x_3^{(m-1)}}{a_{11}}\n",
        "&\n",
        "\\boxed{x_1^{(m)}}=\n",
        "\\frac{b_1-a_{12}x_2^{(m-1)}-a_{13}x_3^{(m-1)}}{a_{11}}\n",
        "\\\\[10pt]\n",
        "\n",
        "\\boxed{x_2^{(m)}}=\n",
        "\\frac{b_2-a_{21}x_1^{(m-1)}-a_{23}x_3^{(m-1)}}{a_{22}}\n",
        "&\n",
        "\\boxed{x_2^{(m)}}=\n",
        "\\frac{b_2-a_{21}\\boxed{x_1^{(m)}}-a_{23}x_3^{(m-1)}}{a_{22}}\n",
        "\\\\[10pt]\n",
        "\n",
        "\\boxed{x_3^{(m)}}=\n",
        "\\frac{b_3-a_{31}x_1^{(m-1)}-a_{32}x_2^{(m-1)}}{a_{33}}\n",
        "&\n",
        "\\boxed{x_3^{(m)}}=\n",
        "\\frac{b_3-a_{31}\\boxed{x_1^{(m)}}-a_{32}\\boxed{x_2^{(m)}}}{a_{33}}\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Key difference\n",
        "- Jacobi uses only $\\mathbf{x}^{(m-1)}$\n",
        "- Gauss–Seidel reuses the newest values immediately"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d82dc17",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Method 3: Conjugate Gradient (CG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cg-mention-slide",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "For **large, sparse, SPD** systems, CG is a standard choice:\n",
        "- can converge in far fewer iterations than Jacobi/GS\n",
        "- performance depends heavily on **preconditioning**\n",
        "\n",
        "> We will not cover this method in detail. Section 11.3.2 of McGuire text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f962af8f",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Choosing Convergence Criteria\n",
        "\n",
        "> Iterative methods are repeated until the solution is “close enough” to the true solution."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa56dc25",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Convergence Criterion — Relative Error\n",
        "\n",
        "A commonly used convergence criterion is\n",
        "$$\n",
        "\\varepsilon_a < \\zeta\n",
        "$$\n",
        "\n",
        "where the approximate relative error is defined as\n",
        "$$\n",
        "\\varepsilon_a\n",
        "=\n",
        "\\max_{i=1,\\ldots,n}\n",
        "\\left|\n",
        "\\frac{x_i^{(m)} - x_i^{(m-1)}}{x_i^{(m)}}\n",
        "\\right|\n",
        "\\times 100\\%.\n",
        "$$\n",
        "\n",
        "Tolerance\n",
        "$$\n",
        "\\zeta = 0.5 \\times 10^{\\,2-q}\\ \\%\n",
        "$$\n",
        "\n",
        "where $q$ is the desired number of correct significant figures.\n",
        "\n",
        "Notes\n",
        "- convergence depends on the initial guess\n",
        "- tighter tolerances require more iterations\n",
        "- for positive definite matrices, convergence is guaranteed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8a2a807",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "### Convergence Criterion — Residual Norm\n",
        "\n",
        "An alternative (and often more physically meaningful) convergence\n",
        "criterion is based on the **residual**.\n",
        "\n",
        "The residual vector is defined as\n",
        "$$\n",
        "\\mathbf{r}^{(m)} = \\mathbf{A}\\mathbf{x}^{(m)} - \\mathbf{b}.\n",
        "$$\n",
        "\n",
        "A commonly used stopping criterion is\n",
        "$$\n",
        "\\|\\mathbf{r}^{(m)}\\| < \\varepsilon_r,\n",
        "$$\n",
        "where $\\|\\cdot\\|$ is typically the Euclidean (2-)norm.\n",
        "\n",
        "\n",
        "Notes\n",
        "- The residual measures **how well the current iterate satisfies the equations**\n",
        "- $\\mathbf{r}=\\mathbf{0}$ corresponds to an exact solution\n",
        "- A small residual means the solution is **nearly in equilibrium**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48812e64",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "The **Euclidean (2-)norm** measures the magnitude of a vector as the square root of the sum of the squares of its components:\n",
        "$$\n",
        "\\|\\mathbf{r}\\|_2 = \\sqrt{r_1^2 + r_2^2 + \\cdots + r_n^2}.\n",
        "$$\n",
        "\n",
        "In practice, both **relative change** and **residual norms**\n",
        "are often monitored together."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbd55c76",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Relaxation and Over-Relaxation\n",
        "\n",
        "> To improve convergence behavior, relaxation schemes may be applied."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6248c0b6",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "After computing a new iterate $x_i^{(m)}$, the value is modified (weighted average)\n",
        "$$\n",
        "x_i^{(m)}\n",
        "\\leftarrow\n",
        "\\beta\\,x_i^{(m)} + (1-\\beta)\\,x_i^{(m-1)}\n",
        "$$\n",
        "\n",
        "Relaxation parameter $\\beta$\n",
        "- $0 < \\beta < 1$  → **under-relaxation**  \n",
        "  (damps oscillations, stabilizes convergence)\n",
        "- $\\beta = 1$      → no relaxation (standard method)\n",
        "- $1 < \\beta < 2$  → **over-relaxation**  \n",
        "  (can significantly accelerate convergence)\n",
        "\n",
        "Notes\n",
        "- over-relaxation is widely used in structural analysis\n",
        "- optimal $\\beta$ is problem-dependent\n",
        "- improper choice of $\\beta$ may cause divergence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wrap-up-slide",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Wrap-Up: Choosing a Solver\n",
        "\n",
        "Given $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$, ask:\n",
        "\n",
        "1. **Is $\\mathbf{A}$ symmetric?**  \n",
        "   - Yes → consider LDL$^T$ or Cholesky\n",
        "\n",
        "2. **Is $\\mathbf{A}$ symmetric positive definite (SPD)?**  \n",
        "   - Yes → Cholesky (direct), Conjugate Gradient (iterative)\n",
        "\n",
        "3. **Is $\\mathbf{A}$ tridiagonal, banded, or sparse?**  \n",
        "   - Yes → specialized solvers  \n",
        "     (Thomas algorithm, banded solvers, frontal methods)\n",
        "\n",
        "4. **Do I have many right-hand sides $\\mathbf{b}$?**  \n",
        "   - Yes → factorize once (LU / LDL$^T$ / Cholesky) and reuse substitutions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba4b8d3b",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Looking Ahead\n",
        "\n",
        "Next week, we begin assembling **stiffness matrices** for structural systems  \n",
        "and applying these linear solvers to real structural workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "part4-title",
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        }
      },
      "source": [
        "## Part 4 — Structure: Bandedness and Frontal Solvers\n",
        "\n",
        "> Structure is where structural engineering gets you big computational wins.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bandedness-slide",
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        }
      },
      "source": [
        "### Bandedness\n",
        "\n",
        "Section 11.4 of McGuire Text, Section 9.9 of Kassimali\n",
        "\n",
        "A matrix is **banded** if nonzeros are concentrated near the diagonal.\n",
        "\n",
        "Bandwidth intuition:\n",
        "- local connectivity (adjacent DOFs) → narrow band\n",
        "- long-range coupling → wide band\n",
        "\n",
        "Why bandedness matters:\n",
        "- less memory\n",
        "- less work in factorization\n",
        "- much smaller fill-in for well-ordered DOFs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "frontal-slide",
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        }
      },
      "source": [
        "### Frontal (and Multifrontals) — The Structural FE View\n",
        "\n",
        "Section 11.5 of McGuire\n",
        "\n",
        "In finite element assembly, you do not have to build the full dense $\\mathbf{K}$ to solve.\n",
        "\n",
        "Frontal idea (high-level):\n",
        "- assemble elements gradually\n",
        "- eliminate DOFs as soon as they become \"fully assembled\"\n",
        "- keep only an active \"front\" (a smaller working matrix)\n",
        "\n",
        "Why this is useful:\n",
        "- reduced peak memory\n",
        "- exploits sparse/banded structure\n",
        "- bridges the FE assembly process and linear algebra\n",
        "\n",
        "> Takeaway: ordering of DOFs and element connectivity heavily controls solver cost.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "CEE6501-student",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "title": "Lecture 2.2 — Solving Linear Systems"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
